{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11209629,"sourceType":"datasetVersion","datasetId":6999576},{"sourceId":11211289,"sourceType":"datasetVersion","datasetId":7000570},{"sourceId":12107693,"sourceType":"datasetVersion","datasetId":7622885}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sacremoses","metadata":{"id":"2r4xvvk6GM-6","executionInfo":{"status":"ok","timestamp":1743161104481,"user_tz":-120,"elapsed":3662,"user":{"displayName":"hazem omran","userId":"04300143560757773047"}},"outputId":"7f4768dd-0cd5-45e9-f31d-2971c09f0ecb","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:37.794898Z","iopub.execute_input":"2025-06-26T19:48:37.795127Z","iopub.status.idle":"2025-06-26T19:48:42.403399Z","shell.execute_reply.started":"2025-06-26T19:48:37.795106Z","shell.execute_reply":"2025-06-26T19:48:42.402306Z"}},"outputs":[{"name":"stdout","text":"Collecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.11.6)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.67.1)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport glob\nimport re\nimport json\nimport string\nimport itertools\n\n\nfrom sacremoses import MosesTokenizer, MosesDetokenizer\n\n# -------------------HERE------------------\n\n# # Run this for me\nINPUT_FILE = \"/kaggle/input/d/amerasaad/transcripts/Bdb001.interaction.txt\"\n\n# OUTPUT_DIR = \"/kaggle/working/preprocessed_data/\"\n\n# OUTPUT_FILE = \"preprocessed_trunscript\"\n\n# Run this for you\n\nBASE_PATH = os.getcwd()\n# INPUT_FILE = os.path.join(BASE_PATH, 'transcripts', 'Bdb001.interaction.txt')\nOUTPUT_DIR = os.path.join(BASE_PATH, 'preprocessed_data')\nOUTPUT_FILE = 'preprocessed_trunscript'\n# -----------------------------------------","metadata":{"id":"9qXHSVde3RuD","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:42.404712Z","iopub.execute_input":"2025-06-26T19:48:42.405006Z","iopub.status.idle":"2025-06-26T19:48:42.743892Z","shell.execute_reply.started":"2025-06-26T19:48:42.404974Z","shell.execute_reply":"2025-06-26T19:48:42.743031Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def read_transcripts(meetings_file):\n    transcript = \"\"\n\n    with open(meetings_file, \"r\", encoding=\"utf-8\") as f:\n        transcript = f.read().splitlines()\n\n    return transcript","metadata":{"pycharm":{"name":"#%%\n"},"id":"vOZSfPov3RuG","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:42.745326Z","iopub.execute_input":"2025-06-26T19:48:42.745542Z","iopub.status.idle":"2025-06-26T19:48:42.749097Z","shell.execute_reply.started":"2025-06-26T19:48:42.745525Z","shell.execute_reply":"2025-06-26T19:48:42.748327Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"transcript = read_transcripts(INPUT_FILE)\n# en_automin2023 = read_transcripts(os.path.join(ELITR_AUTOMIN_2023_DIR, AUTOMIN_EN_DIR))","metadata":{"pycharm":{"name":"#%%\n"},"id":"fB470_Pr3RuG","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:42.750118Z","iopub.execute_input":"2025-06-26T19:48:42.750369Z","iopub.status.idle":"2025-06-26T19:48:42.770504Z","shell.execute_reply.started":"2025-06-26T19:48:42.750343Z","shell.execute_reply":"2025-06-26T19:48:42.769865Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"transcript","metadata":{"id":"UX6GHovQH9Me","executionInfo":{"status":"ok","timestamp":1743163461758,"user_tz":-120,"elapsed":28,"user":{"displayName":"hazem omran","userId":"04300143560757773047"}},"outputId":"f436a06e-32c8-42a1-dbff-1489328ed924","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:42.771149Z","iopub.execute_input":"2025-06-26T19:48:42.771370Z","iopub.status.idle":"2025-06-26T19:48:42.780777Z","shell.execute_reply.started":"2025-06-26T19:48:42.771352Z","shell.execute_reply":"2025-06-26T19:48:42.780115Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[\"\\ufeffSpeaker 1: Yeah, we had a long discussion about how easy we want to make it for people to bleep things out.  Morgan wants to make it hard.  I didn't even check yesterday.  \",\n '',\n \"Speaker 0: It didn't move yesterday either when I started it.  I don't know if it doesn't like both of them.  \",\n '',\n 'Speaker 1: Channel 3?  ',\n '',\n \"Speaker 0: You know, I discovered something yesterday on these wireless ones.  You can tell if it's picking up breath noise and stuff.  \",\n '',\n 'Speaker 1: Yeah, it has a little indicator on it, on the AF.  ',\n '',\n \"Speaker 0: So if you breathe under, breathe, and then you see AF go off, then you know it's picking up your mouth noise.  \",\n '',\n \"Speaker 2: Oh, that's good, because we have a lot of breath tests.  In fact, if you listen to just the channels of people not talking, it's like...  \",\n '',\n 'Speaker 1: What, did you see Hannibal recently or something?  Exactly.  ',\n '',\n \"Speaker 2: Very disconcerting.  Okay.  So I was going to try to get out of here in half an hour because I really appreciate people coming.  And the main thing that I was going to ask people to help with today is to... give input on what kinds of database format we should use in starting to link up things like word transcripts and annotations of word transcripts, so anything that transcribers or discourse coders would ever put in the signal with time marks for like words and phone boundaries and all the stuff we get out of the forced alignments in the recognizer.  So we have this, I think a starting point is clearly the the channelized output of Dave Gelbart's program, which Don brought a copy of.  \",\n '',\n \"Speaker 1: Yeah, I'm familiar with that.  I mean, I sort of already have developed an XML format for this sort of stuff.  And so the only question, is it the sort of thing that you want to use or not?  Have you looked at that?  I mean, I had a web page up.  Right.  \",\n '',\n \"Speaker 2: So I actually mostly need to be able to link up or... It's a question both of what the representation is.  \",\n '',\n 'Speaker 1: I guess I am going to be standing up and drawing on the board.  ',\n '',\n 'Speaker 2: Okay, yeah, so you should definitely.  ',\n '',\n \"Speaker 1: So it definitely had that as a concept.  So it has a single timeline, and then you can have lots of different sections, each of which have IDs attached to it, and then you can refer from other sections to those IDs if you want to.  So that you start with a timeline tag.  Timeline.  And then you have a bunch of times.  I don't remember exactly what my notation was, but...  \",\n '',\n 'Speaker 3: Right, right.  ',\n '',\n \"Speaker 1: Yeah, t equals 1.32.  And then I also had optional things like accuracy, and then id equals t10.  And then I also wanted to be able to not specify specifically what the time was and just have a stamp.  Yeah, so these are arbitrary assigned by a program, not by a user.  So you have a whole bunch of those.  And then somewhere further down, you might have something like an utterance tag, which has start equals T17, end equals T18.  So what that's saying is we know it starts at this particular time.  We don't know when it ends.  Okay.  Right?  But it ends at this T18, which may be somewhere else.  We say there's another utterance.  We don't know what the time actually is, but we know that it's the same time as this end time.  Yeah, 38, whatever you want.  \",\n '',\n \"Speaker 3: So you're essentially defining a lattice.  \",\n '',\n \"Speaker 1: Yes, exactly.  And then these also have IDs.  Right?  So you could have some sort of other tag later in the file.  that would be something like... Oh, I don't know.  Noise type equals door slam, you know.  And then you could either say time equals a particular time mark, or you could do other sorts of references.  Or you might have a prosody.  Prosody, right?  D, T?  \",\n '',\n \"Speaker 2: It's an O instead of an I, but the D is good.  \",\n '',\n \"Speaker 1: You like the D?  That's a good D. So you could have some sort of type here, and then you could have the utterance that it's referring to could be U17 or something like that.  \",\n '',\n \"Speaker 2: Okay, so that seems great for all of the encoding of things with time.  I guess my question is more... What do you do with, say, a forced alignment when you've got all these phone labels?  And what do you do if you, just conceptually, if you get transcriptions where the words are staying but the time boundaries are changing because you've got a new recognition output?  What's the sequence of going from the waveforms that stay the same, the transcripts that may or may not change, and then the... utterance which are the time boundaries that may or may not change?  \",\n '',\n \"Speaker 3: that's actually very nicely handled here because you could you could.  all you'd have to change is the time stamps in the timeline right.  \",\n '',\n \"Speaker 1: that's that's why you do that extra level of indirection so that you can just change the timeline Except the timeline is going to be huge.  Yes.  \",\n '',\n 'Speaker 2: Yeah, especially at the phone level.  ',\n '',\n 'Speaker 3: Suppose you have a phone level alignment.  ',\n '',\n \"Speaker 1: Yeah, I don't think I would do this for phone level.  I think for phone level you want to use some sort of binary representation because it'll be too dense otherwise.  \",\n '',\n 'Speaker 2: Okay, so if you were doing that and you had this sort of companion thing that gets called up for phone level, what would that look like?  ',\n '',\n 'Speaker 1: I would use just an existing way of doing it.  ',\n '',\n \"Speaker 3: It's just a matter of it being bigger.  But if you have... You know, barring memory limitations, I mean, this is still...  \",\n '',\n \"Speaker 1: It's parsing limitations.  I don't want to have this text file that you have to read in the whole thing to do something very simple for.  \",\n '',\n 'Speaker 3: Oh, no, you would use it only for purposes where you actually want the phone level information.  ',\n '',\n 'Speaker 2: So you could have some file that configures how much information you want in your XML or something.  ',\n '',\n \"Speaker 1: Right.  I mean, I imagine you'd have multiple versions of this depending on the information that you want.  I'm just... What I'm wondering is whether, I think for word level, this would be okay.  For word level, it's all right.  For lower than word level, you're talking about so much data that I just, I don't know.  I don't know if that.  \",\n '',\n 'Speaker 2: I mean, we actually have, so one thing that Don is doing.  ',\n '',\n 'Speaker 0: Lattices are big too.  ',\n '',\n \"Speaker 2: We're running, for every frame, you get a pitch value.  And not only one pitch value, but different kinds of pitch values.  \",\n '',\n 'Speaker 1: Yeah, I mean, for something like that, I would use pfile.  Or any frame level stuff, I would use pfile.  ',\n '',\n 'Speaker 2: Meaning?  ',\n '',\n \"Speaker 1: That's a, well, or something like it.  ICSI has a format for frame level representation of features.  \",\n '',\n 'Speaker 2: Okay, that you could call, that you would tie into this representation with like an ID.  ',\n '',\n \"Speaker 1: Right, or there's a particular way in XML to refer to external resources.  \",\n '',\n 'Speaker 2: Okay.  ',\n '',\n \"Speaker 1: So you would say, refer to this external file.  So that external file wouldn't be in.  \",\n '',\n \"Speaker 0: But what's the advantage of doing that versus just putting it into this format?  More compact.  \",\n '',\n \"Speaker 1: Which I think is better.  I mean, if you did it at this... You don't want to do it with that.  Anything at frame level, you had better in code binary or it's going to be really painful.  \",\n '',\n 'Speaker 3: I mean, I like text formats.  You can always gzip them and decompress them on the fly if space is really...  ',\n '',\n 'Speaker 0: I was thinking the advantage is that we can share this with other people.  ',\n '',\n \"Speaker 1: You're talking about gigabyte-sized files.  You're going to actually run out of space in your file system for one file.  Right, because you have a two gigabyte limit on most OSs.  \",\n '',\n \"Speaker 3: I would say, okay, so frame level is probably not a good idea.  But for phone level stuff, it's perfectly, like phones or syllables.  \",\n '',\n 'Speaker 2: Phones are every five frames though, so something like that.  ',\n '',\n \"Speaker 3: But most of the frames are actually not speech.  So, you know, people don't.  \",\n '',\n 'Speaker 2: Yeah, but we actually.  ',\n '',\n \"Speaker 3: Look at it, words times the average number of phones in an English word is, I don't know, five maybe.  So look at it, number of words times five.  \",\n '',\n 'Speaker 2: So you mean pause phones take up a lot of the, long pause phones.  ',\n '',\n 'Speaker 1: Yeah.  Exactly.  ',\n '',\n \"Speaker 2: Okay, that's true, but you do have to keep them in there.  \",\n '',\n \"Speaker 1: So I think it's debatable whether you want to do phone level in the same thing, but I think anything at frame level, even p-file is too verbose.  I would use something tighter than p-files.  \",\n '',\n \"Speaker 2: Are you familiar with it?  I haven't seen this particular format.  \",\n '',\n \"Speaker 3: I've used them.  \",\n '',\n \"Speaker 0: I don't know what their structure is.  P-file for each frame is storing a vector of, Kepstroll or PLP values, right?  \",\n '',\n \"Speaker 1: It's whatever you want, actually.  So what's nice about the P file, built into it is the concept of frames, utterances, sentences, that sort of thing, that structure.  And then also attached to it is an arbitrary vector of values.  And it can take different types, so they don't all have to be floats.  You can have integers and you can have doubles and all that sort of stuff.  \",\n '',\n 'Speaker 2: That sounds about what I...  ',\n '',\n \"Speaker 1: Right, and it has a header format that... describes it to some extent.  so the only problem with it is it's actually storing the utterance numbers and the frame numbers in the file even though they're always sequential and so it does waste a lot of space.  but it's still a lot tighter than than ASCII and we have a lot of tools already to deal with it.  \",\n '',\n 'Speaker 2: okay is there some documentation?  ',\n '',\n \"Speaker 1: yeah there's a ton of it man pages and source code and I mean, that sounds good.  \",\n '',\n \"Speaker 2: I was just looking for something.  I'm not a database person, but something sort of standard enough that, you know, if we start using this, we can give it out, other people can work on it.  \",\n '',\n \"Speaker 1: Yeah, it's not standard.  I mean, it's something that we developed at ICSI.  \",\n '',\n \"Speaker 2: But it's been used here.  \",\n '',\n \"Speaker 1: But it's been used here, and, you know, we have a well-configured system that you can distribute for free.  \",\n '',\n 'Speaker 0: I mean, it must be the equivalent of whatever you guys use to store your... computed features in, right?  ',\n '',\n \"Speaker 3: Yeah, we have... Actually, we use a generalization of the sphere format.  But, yeah, so there's something like that, but it's...  \",\n '',\n 'Speaker 1: And I think, what does HTK do for features?  Or does it even have a concept of features?  ',\n '',\n \"Speaker 3: I mean, Entropic has their own feature format.  It's called, like, S. SD or something, SF or something like that.  \",\n '',\n \"Speaker 1: I'm just wondering, would it be worthwhile to use that instead?  \",\n '',\n 'Speaker 2: Yeah, this is exactly the kind of decision.  ',\n '',\n \"Speaker 0: People don't typically share this kind of stuff, right?  \",\n '',\n 'Speaker 1: They generate their own.  ',\n '',\n \"Speaker 2: Actually, we've done this stuff on prosodics, and three or four places have asked for those prosodic files, and we just have an ASCII output of frame by frame, which is fine.  But it gets unwieldy to go in and query these files with really huge files.  \",\n '',\n 'Speaker 1: Right.  ',\n '',\n \"Speaker 2: And we could do it.  I was just thinking if there's something that...  \",\n '',\n \"Speaker 1: And again, if you have a... Where all the frame values are.  If you have a two-hour long meeting, that's going to...  \",\n '',\n \"Speaker 2: They're quite large.  \",\n '',\n \"Speaker 1: Yeah, I mean, they'd be enormous.  \",\n '',\n \"Speaker 2: These are for 10-minute switchboard conversations.  Right.  So it's doable.  It's just that you can only store a feature vector frame by frame, and it doesn't have any kind of...  \",\n '',\n 'Speaker 0: Is the sharing part of this?  a pretty important consideration, or is that just sort of a nice thing to have?  ',\n '',\n \"Speaker 2: I don't know enough about what we're going to do with the data, but I thought it would be good to get something that other people can use or adopt for their own kinds of encoding.  We have to make some decision about what to do, and especially for the prosody work, what it ends up being is you get... features from the signal and of course those change every time your alignments change.  so you rerun a recognizer you want to recompute your features and then keep the database up to date.  or you change a word or you change a utterance boundary segment which is going to happen a lot.  and so I wanted something where all of this can be done in an elegant way and that if somebody wants to try something or compute something else that it can be done flexibly.  it doesn't have to be pretty it just has to be easy to use.  \",\n '',\n \"Speaker 1: and yeah the other thing we should look at atlas the nist thing and see if they have anything at that level.  i mean i'm not sure what to do about this with atlas because they chose a different route.  i chose something that there are sort of two choices your your file format can know about know that you're talking about language and speech, which is what I chose, and time, or your file format can just be a graph representation.  And then the application has to impose the structure on top.  So what it looked like Atlas chose is they chose the other way, which was their file format is just nodes and links, and you have to interpret what they mean yourself.  \",\n '',\n 'Speaker 2: And why did you not choose that?  ',\n '',\n \"Speaker 1: Because I knew that we were doing speech, and I thought it was better, if you're looking at a raw file, for the tags to say it's an utterance, as opposed to the tag to say, It's a link.  \",\n '',\n 'Speaker 2: Okay.  But other than that, are they compatible?  I mean, you could sort of...  ',\n '',\n \"Speaker 1: Yeah, they're reasonably compatible.  \",\n '',\n 'Speaker 0: You could probably translate between them.  ',\n '',\n \"Speaker 2: Yeah, that's... So...  \",\n '',\n 'Speaker 1: So, well, the other thing is if we choose to use Atlas, which maybe we should just do, we should just throw this out before we invest a lot of time in it.  ',\n '',\n \"Speaker 2: So this is what the meeting's about, just sort of how to... Because we need to come up with a database like this just to do our work.  And I actually don't care as long as it's something useful to other people, what we choose.  So maybe it's... If you have any idea of how to choose, because I don't.  \",\n '',\n \"Speaker 1: Yeah.  I chose this for a couple reasons.  One of them is that it's easy to parse.  You don't need a full XML parser.  It's very easy to just write a Perl script to parse it.  \",\n '',\n 'Speaker 3: As long as each tag is on one line.  ',\n '',\n 'Speaker 1: Exactly, exactly, which I always do.  ',\n '',\n 'Speaker 2: And you can have as much information in the tag as you want, right?  ',\n '',\n 'Speaker 1: Well, I have it structured, right?  So each tag has only particular items that it can take.  ',\n '',\n 'Speaker 2: But you can add to those structures if you have more information.  ',\n '',\n \"Speaker 1: So what NIST would say is that instead of doing this, you would say something like link start equals some node ID and equals some other node ID.  And then type would be utterance.  So it's very similar.  \",\n '',\n \"Speaker 2: So why would it be a waste to do it this way if it's similar enough that we can always?  \",\n '',\n \"Speaker 0: Probably wouldn't be a waste.  it would mean that at some point if we wanted to switch We just have to write a translator.  \",\n '',\n \"Speaker 1: fancy since they're developing a big.  \",\n '',\n \"Speaker 0: I don't think that's a big deal.  \",\n '',\n \"Speaker 1: They're developing a big infrastructure And so it seems to me that if if we want to use that we might as well go directly to what they're doing rather than Yes, see that's the question.  \",\n '',\n 'Speaker 0: I mean how stable is there?  are they ready to go?  ',\n '',\n \"Speaker 1: I looked at it.  The last time I looked at it was a while ago, probably a year ago, when we first started talking about this.  And at that time, at least, it was still not very complete.  And so, specifically, they didn't have any external format representation at that time.  They just had sort of conceptual node annotated transcription graph, which I really liked.  And that's exactly what this stuff is based on.  Since then, they've developed their own external file format, which is... You know, this sort of thing.  And apparently they've also developed a lot of tools, but I haven't looked at them.  Maybe I should.  \",\n '',\n 'Speaker 2: Would the tools run on something like this, if you can translate them in any way?  ',\n '',\n 'Speaker 1: What would worry me is that maybe we might miss a little detail.  That would make it very difficult to translate from one to the other.  ',\n '',\n 'Speaker 3: they already have or will have tools that everybody else will be using.  ',\n '',\n 'Speaker 1: Yeah, we might as well.  ',\n '',\n 'Speaker 3: It would be crazy to do something separate.  ',\n '',\n \"Speaker 1: So I'll take a closer look at it.  \",\n '',\n \"Speaker 2: That would really be the question, just what you feel is, in the long run, the best thing.  Because once we start doing this, we don't actually have enough time to probably have to rehash it out again.  \",\n '',\n 'Speaker 1: The other thing... The other way that I sort of established this was as easy translation to and from the transcriber format.  Right, right.  ',\n '',\n 'Speaker 2: I mean, I like this.  This is sort of intuitively easy to actually read, as easy as it could be.  But I suppose that as long as they have a type here that specifies...  ',\n '',\n \"Speaker 1: It's almost the same.  The point with this, though, is that you can't really add any supplementary information.  Right, so if you suddenly decide that you want...  \",\n '',\n 'Speaker 2: You have to make a different type.  ',\n '',\n \"Speaker 1: Yeah, you'd have to make a different type.  \",\n '',\n \"Speaker 2: So, well, if you look at it, and I guess in my mind, I don't know enough, Jane would know better about the types of annotations, but I imagine that those are things that would, well, you guys mentioned this, that could span any, it could be in its own channel, it could span time boundaries of any type, it could be instantaneous, things like that.  And then from the recognition side, we have backtraces at the phone level.  If it can handle that, it could handle states or whatever.  And then at the prosody level, we have frame sort of like actual feature files, like these p-files or anything like that.  And that's sort of the world of things that I... And then we have the aligned channels, of course.  \",\n '',\n 'Speaker 3: It seems to me you want to keep the frame level stuff separate.  ',\n '',\n \"Speaker 2: Yeah, I definitely agree, and I wanted to find actually a nicer format or maybe a more compact format than what we used before, just because you've got 10 channels or whatever and two hours of a meeting.  \",\n '',\n 'Speaker 3: Now, how would you represent multiple speakers in this framework?  You would just represent them as... You would have like a speaker tag or something?  ',\n '',\n \"Speaker 1: There's a speaker tag up at the top which identifies them, and then the way I had it is each churn or each utterance, I don't even remember now, had a speaker ID tag attached to it.  Okay.  In this format, you would have a different tag, which would be linked to the link.  Yeah.  So somewhere else you would have another thing that would be, let's see, would it be a node or a link?  And so this one would have an ID, is link 74 or something like that.  And then somewhere up here you would have a link that, you know, was referencing L74 and had speaker Adam, you know, or something like that.  \",\n '',\n 'Speaker 2: I mean, yeah, channel is what the channelized output is.  ',\n '',\n \"Speaker 1: This isn't quite right.  I have to look at it again.  \",\n '']"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import re\n\ndef parse_transcript_by_speaker(transcript):\n    parsed_transcript = []\n\n    for line in transcript:\n        # Match lines starting with \"Speaker X:\"\n        match = re.match(r\"^Speaker\\s+(\\d+):\\s*(.*)\", line)\n        if match:\n            speaker_id, utterance = match.group(1), match.group(2).strip()\n            parsed_transcript.append({\n                \"role\": f\"Speaker {speaker_id}\",\n                \"utterance\": [utterance]\n            })\n        elif len(parsed_transcript) > 0:\n            # Continuation of previous speaker\n            parsed_transcript[-1][\"utterance\"].append(line.strip())\n\n    return parsed_transcript\n","metadata":{"pycharm":{"name":"#%%\n"},"id":"vAZS9MUY3RuH","trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-06-26T19:48:42.781429Z","iopub.execute_input":"2025-06-26T19:48:42.781700Z","iopub.status.idle":"2025-06-26T19:48:42.796654Z","shell.execute_reply.started":"2025-06-26T19:48:42.781682Z","shell.execute_reply":"2025-06-26T19:48:42.796039Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\nimport string\nimport itertools\nfrom sacremoses import MosesTokenizer, MosesDetokenizer\n\ndef remove_asr_errors(tokens):\n    # Add ^ and $ to anchor patterns to full token matches\n    ASR_STOPWORDS = [\n        r\"^u+h+m*-?$\", r\"^m*h+m+-?$\", r\"^u+m+-?$\",\n        r\"^e+h+m*-?$\", r\"^e*m+-?$\", r\"^e+r+m+-?$\",\n        r\"^a+h+$\", r\"^u+h+n+-?$\", r\"^h+u+(h|m)+-?$\"\n    ]\n    ASR_STOPWORDS_COMBINATIONS = [\n        f\"{stop0[:-1]}-{stop1[1:]}\"  # strip anchors when combining\n        for stop0, stop1 in itertools.combinations(ASR_STOPWORDS, 2)\n    ]\n\n    # Remove ASR stopwords\n    filtered_tokens = [\n        token for token in tokens\n        if not any(re.fullmatch(regex, token.lower()) for regex in ASR_STOPWORDS + ASR_STOPWORDS_COMBINATIONS)\n    ]\n\n    # Handle dangling hyphens\n    filtered_tokens2 = []\n    for idx, token in enumerate(filtered_tokens):\n        if token == \"-\" or not token.endswith(\"-\"):\n            filtered_tokens2.append(token)\n        elif idx == len(filtered_tokens)-1 or not filtered_tokens[idx+1].lower().startswith(token[:-1].lower()):\n            filtered_tokens2.append(token[:-1])\n\n    return filtered_tokens2\n\ndef remove_tags(text):\n    text = re.sub(r\"<.*?>\", \"\", text)\n    text = re.sub(r\"\\(?\\)\", \"\", text)\n    text = re.sub(r\"[\\[\\]()]\", \"\", text)\n    return text\n\ndef is_punct(s):\n    return all(c in string.punctuation + \"–\" for c in s)\n\ndef normalize_text(text):\n    text = remove_tags(text)\n\n    tokenizer = MosesTokenizer(lang=\"en\")\n    detokenizer = MosesDetokenizer(lang=\"en\")\n\n    tokens = tokenizer.tokenize(text)\n    tokens = remove_asr_errors(tokens)\n\n    # Remove punctuation at start\n    try:\n        first_non_punct_idx = next(idx for idx, token in enumerate(tokens) if not is_punct(token))\n        tokens = tokens[first_non_punct_idx:]\n    except StopIteration:\n        tokens = []\n\n    if tokens:\n        # Remove consecutive duplicates (case-insensitive)\n        tokens = [\n            token for idx, token in enumerate(tokens)\n            if idx == 0 or token.lower() != tokens[idx - 1].lower()\n        ]\n\n        # Remove consecutive punctuations\n        tokens = [\n            token for idx, token in enumerate(tokens)\n            if idx == len(tokens) - 1 or not is_punct(tokens[idx]) or not is_punct(tokens[idx + 1])\n        ]\n\n        # Capitalize first word if it's alphabetic\n        if tokens[0].isalpha():\n            tokens[0] = tokens[0][0].upper() + tokens[0][1:]\n\n        # End sentence with period if it doesn't already\n        if not is_punct(tokens[-1][-1]):\n            tokens.append(\".\")\n\n    return detokenizer.detokenize(tokens)\n","metadata":{"pycharm":{"name":"#%%\n"},"id":"v7-OyoPE3RuH","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:42.797503Z","iopub.execute_input":"2025-06-26T19:48:42.797771Z","iopub.status.idle":"2025-06-26T19:48:42.813745Z","shell.execute_reply.started":"2025-06-26T19:48:42.797746Z","shell.execute_reply":"2025-06-26T19:48:42.812914Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_transcript(transcript):\n    roles = []\n    utterances = []\n\n\n    for line in transcript:\n        normalized_utterance = [normalize_text(sentence) for sentence in line[\"utterance\"]]\n        normalized_utterance = \" \".join(sentence for sentence in normalized_utterance if len(sentence) > 0)\n\n        if len(normalized_utterance) > 0:\n            roles.append(line[\"role\"])\n            utterances.append(normalized_utterance)\n\n    assert len(roles) == len(utterances)\n    return {\"roles\": roles, \"utterances\": utterances}","metadata":{"pycharm":{"name":"#%%\n"},"id":"66jwodmP3RuH","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:42.816085Z","iopub.execute_input":"2025-06-26T19:48:42.816274Z","iopub.status.idle":"2025-06-26T19:48:42.832286Z","shell.execute_reply.started":"2025-06-26T19:48:42.816257Z","shell.execute_reply":"2025-06-26T19:48:42.831691Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def preprocess_transcripts(transcripts):\n    preprocessed_transcripts = {}\n\n    for meeting_id, transcript in transcripts.items():\n        preprocessed_transcript = parse_transcript_by_speaker(transcript)\n        preprocessed_transcripts[meeting_id] = preprocess_transcript(preprocessed_transcript)\n\n    return preprocessed_transcripts","metadata":{"pycharm":{"name":"#%%\n"},"id":"ebKusvWj3RuI","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:42.833571Z","iopub.execute_input":"2025-06-26T19:48:42.833773Z","iopub.status.idle":"2025-06-26T19:48:42.847649Z","shell.execute_reply.started":"2025-06-26T19:48:42.833755Z","shell.execute_reply":"2025-06-26T19:48:42.846869Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"parsed_transcript = parse_transcript_by_speaker(transcript)\ninput_preprocessed = preprocess_transcript(parsed_transcript)\n","metadata":{"pycharm":{"name":"#%%\n"},"id":"iyX4xgnM3RuI","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:42.848454Z","iopub.execute_input":"2025-06-26T19:48:42.848708Z","iopub.status.idle":"2025-06-26T19:48:43.104787Z","shell.execute_reply.started":"2025-06-26T19:48:42.848683Z","shell.execute_reply":"2025-06-26T19:48:43.104201Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def save_preprocessed(preprocessed):\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    with open(os.path.join(OUTPUT_DIR, f\"{OUTPUT_FILE}.json\"), \"w\") as f:\n        json.dump(preprocessed, f, ensure_ascii=False, indent=4)","metadata":{"pycharm":{"name":"#%%\n"},"id":"uCSy-5wm3RuI","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:43.105519Z","iopub.execute_input":"2025-06-26T19:48:43.105740Z","iopub.status.idle":"2025-06-26T19:48:43.109695Z","shell.execute_reply.started":"2025-06-26T19:48:43.105721Z","shell.execute_reply":"2025-06-26T19:48:43.109037Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"save_preprocessed(input_preprocessed)\n","metadata":{"pycharm":{"name":"#%%\n"},"id":"Hp2yz3MM3RuJ","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:43.110423Z","iopub.execute_input":"2025-06-26T19:48:43.110679Z","iopub.status.idle":"2025-06-26T19:48:43.126315Z","shell.execute_reply.started":"2025-06-26T19:48:43.110660Z","shell.execute_reply":"2025-06-26T19:48:43.125701Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# !zip -r preprocessed_data.zip /content/preprocessed_data\n","metadata":{"id":"vBMTlzFPIVLq","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:43.127147Z","iopub.execute_input":"2025-06-26T19:48:43.127412Z","iopub.status.idle":"2025-06-26T19:48:43.140596Z","shell.execute_reply.started":"2025-06-26T19:48:43.127385Z","shell.execute_reply":"2025-06-26T19:48:43.140031Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# -------------------------HERE------------------------\n# Run this for me\nINPUT_DATA_PATH = \"/kaggle/working/preprocessed_data/preprocessed_trunscript.json\"\nOUTPUT_DATA_PATH = \"/kaggle/working/output\"\n\n# Run this for you\n\n# BASE_PATH = os.getcwd()\n# INPUT_DATA_PATH = os.path.join(BASE_PATH, 'preprocessed_data', 'preprocessed_trunscript.json')\n# OUTPUT_DATA_PATH = os.path.join(BASE_PATH, 'output')\n# -----------------------------------------------------\n\n# MODEL_SHORT_NAME = \"bart_large_xsum_samsum\"\n# MODEL_SHORT_NAME = \"MEETING_SUMMARY\"\nMODEL_SHORT_NAME = \"bart-large-cnn-samsum\"\n\n# MODEL = f\"facebook/bart-large-xsum\"\n# MODEL = f\"knkarthick/{MODEL_SHORT_NAME}\"\nMODEL = f\"philschmid/{MODEL_SHORT_NAME}\"\n\n# SUMMARIZER_MODEL = f\"models/{MODEL_SHORT_NAME}/checkpoint-5500\"\nSUMMARIZER_MODEL = MODEL","metadata":{"id":"aAinVZp_I6f4","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:43.141270Z","iopub.execute_input":"2025-06-26T19:48:43.141469Z","iopub.status.idle":"2025-06-26T19:48:43.155710Z","shell.execute_reply.started":"2025-06-26T19:48:43.141451Z","shell.execute_reply":"2025-06-26T19:48:43.155033Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport datetime\nimport json\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport math\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:48:43.156383Z","iopub.execute_input":"2025-06-26T19:48:43.156596Z","iopub.status.idle":"2025-06-26T19:49:03.776981Z","shell.execute_reply.started":"2025-06-26T19:48:43.156568Z","shell.execute_reply":"2025-06-26T19:49:03.776286Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name())\n    torch.cuda.set_device(0)\nelse:\n    print('No GPU available, using the CPU instead.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:03.777738Z","iopub.execute_input":"2025-06-26T19:49:03.778422Z","iopub.status.idle":"2025-06-26T19:49:03.858884Z","shell.execute_reply.started":"2025-06-26T19:49:03.778389Z","shell.execute_reply":"2025-06-26T19:49:03.858035Z"}},"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL)\ndevice = 0 if torch.cuda.is_available() else None\nsummarizer = pipeline(\"summarization\", model=SUMMARIZER_MODEL, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:03.859895Z","iopub.execute_input":"2025-06-26T19:49:03.860230Z","iopub.status.idle":"2025-06-26T19:49:13.712555Z","shell.execute_reply.started":"2025-06-26T19:49:03.860199Z","shell.execute_reply":"2025-06-26T19:49:13.711769Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/300 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe7b69f7e4745c9ae0d071df9b518b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bfd804a107d432095a89c585c6f50e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1b94fb374024303a6d6e6323e1e11a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bcb7e5d30614433b52445135aed2558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c19a49c12b40f6bf3912b129b0c098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff428169177c449ebb5e3468a10188f2"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def load_preprocessed_transcripts(file_name):\n  with open(f\"{file_name}\", \"r\") as f:\n    preprocessed_transcripts = json.load(f)\n\n  return preprocessed_transcripts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:13.713433Z","iopub.execute_input":"2025-06-26T19:49:13.713661Z","iopub.status.idle":"2025-06-26T19:49:13.718655Z","shell.execute_reply.started":"2025-06-26T19:49:13.713639Z","shell.execute_reply":"2025-06-26T19:49:13.717858Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"input_preprocessed = load_preprocessed_transcripts(INPUT_DATA_PATH)\n# europarl_preprocessed = load_preprocessed_transcripts(os.path.join(PREPROCESSED_DIR, EUROPARL_DATA_PATH))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:13.719584Z","iopub.execute_input":"2025-06-26T19:49:13.719860Z","iopub.status.idle":"2025-06-26T19:49:13.839188Z","shell.execute_reply.started":"2025-06-26T19:49:13.719833Z","shell.execute_reply":"2025-06-26T19:49:13.838378Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"input_preprocessed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:13.839970Z","iopub.execute_input":"2025-06-26T19:49:13.840189Z","iopub.status.idle":"2025-06-26T19:49:13.857364Z","shell.execute_reply.started":"2025-06-26T19:49:13.840171Z","shell.execute_reply":"2025-06-26T19:49:13.856702Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'roles': ['Speaker 0',\n  'Speaker 1',\n  'Speaker 0',\n  'Speaker 1',\n  'Speaker 0',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 0',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 0',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 0',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 2',\n  'Speaker 3',\n  'Speaker 2',\n  'Speaker 3',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 3',\n  'Speaker 0',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 0',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 0',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 0',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 0',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 0',\n  'Speaker 1',\n  'Speaker 0',\n  'Speaker 1',\n  'Speaker 0',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 3',\n  'Speaker 2',\n  'Speaker 3',\n  'Speaker 1',\n  'Speaker 2',\n  'Speaker 1'],\n 'utterances': [\"It didn't move yesterday either when I started it. I don't know if it doesn't like both of them.\",\n  'Channel 3?',\n  \"You know, I discovered something yesterday on these wireless ones. You can tell if it's picking up breath noise and stuff.\",\n  'Yeah, it has a little indicator on it, on the AF.',\n  \"So if you breathe under, breathe, and then you see AF go off, then you know it's picking up your mouth noise.\",\n  \"Oh, that's good, because we have a lot of breath tests. In fact, if you listen to just the channels of people not talking, it's like...\",\n  'What, did you see Hannibal recently or something? Exactly.',\n  \"Very disconcerting. Okay. So I was going to try to get out of here in half an hour because I really appreciate people coming. And the main thing that I was going to ask people to help with today is to... give input on what kinds of database format we should use in starting to link up things like word transcripts and annotations of word transcripts, so anything that transcribers or discourse coders would ever put in the signal with time marks for like words and phone boundaries and all the stuff we get out of the forced alignments in the recognizer. So we have this, I think a starting point is clearly the channelized output of Dave Gelbart's program, which Don brought a copy of.\",\n  \"Yeah, I'm familiar with that. I mean, I sort of already have developed an XML format for this sort of stuff. And so the only question, is it the sort of thing that you want to use or not? Have you looked at that? I mean, I had a web page up. Right.\",\n  \"So I actually mostly need to be able to link up or... It's a question both of what the representation is.\",\n  'I guess I am going to be standing up and drawing on the board.',\n  'Okay, yeah, so you should definitely.',\n  \"So it definitely had that as a concept. So it has a single timeline, and then you can have lots of different sections, each of which have IDs attached to it, and then you can refer from other sections to those IDs if you want to. So that you start with a timeline tag. Timeline. And then you have a bunch of times. I don't remember exactly what my notation was, but...\",\n  'Right, right.',\n  \"Yeah, t equals 1.32. And then I also had optional things like accuracy, and then id equals t10. And then I also wanted to be able to not specify specifically what the time was and just have a stamp. Yeah, so these are arbitrary assigned by a program, not by a user. So you have a whole bunch of those. And then somewhere further down, you might have something like an utterance tag, which has start equals T17, end equals T18. So what that's saying is we know it starts at this particular time. We don't know when it ends. Okay. Right? But it ends at this T18, which may be somewhere else. We say there's another utterance. We don't know what the time actually is, but we know that it's the same time as this end time. Yeah, 38, whatever you want.\",\n  \"So you're essentially defining a lattice.\",\n  \"Yes, exactly. And then these also have IDs. Right? So you could have some sort of other tag later in the file. that would be something like... Oh, I don't know. Noise type equals door slam, you know. And then you could either say time equals a particular time mark, or you could do other sorts of references. Or you might have a prosody. Prosody, right? D, T?\",\n  \"It's an O instead of an I, but the D is good.\",\n  \"You like the D? That's a good D. So you could have some sort of type here, and then you could have the utterance that it's referring to could be U17 or something like that.\",\n  \"Okay, so that seems great for all of the encoding of things with time. I guess my question is more... What do you do with, say, a forced alignment when you've got all these phone labels? And what do you do if you, just conceptually, if you get transcriptions where the words are staying but the time boundaries are changing because you've got a new recognition output? What's the sequence of going from the waveforms that stay the same, the transcripts that may or may not change, and then the... utterance which are the time boundaries that may or may not change?\",\n  \"That's actually very nicely handled here because you could you could. all you'd have to change is the time stamps in the timeline right.\",\n  \"That's that's why you do that extra level of indirection so that you can just change the timeline Except the timeline is going to be huge. Yes.\",\n  'Yeah, especially at the phone level.',\n  'Suppose you have a phone level alignment.',\n  \"Yeah, I don't think I would do this for phone level. I think for phone level you want to use some sort of binary representation because it'll be too dense otherwise.\",\n  'Okay, so if you were doing that and you had this sort of companion thing that gets called up for phone level, what would that look like?',\n  'I would use just an existing way of doing it.',\n  \"It's just a matter of it being bigger. But if you have... You know, barring memory limitations, I mean, this is still...\",\n  \"It's parsing limitations. I don't want to have this text file that you have to read in the whole thing to do something very simple for.\",\n  'Oh, no, you would use it only for purposes where you actually want the phone level information.',\n  'So you could have some file that configures how much information you want in your XML or something.',\n  \"Right. I mean, I imagine you'd have multiple versions of this depending on the information that you want. I'm just... What I'm wondering is whether, I think for word level, this would be okay. For word level, it's all right. For lower than word level, you're talking about so much data that I just, I don't know. I don't know if that.\",\n  'I mean, we actually have, so one thing that Don is doing.',\n  'Lattices are big too.',\n  \"We're running, for every frame, you get a pitch value. And not only one pitch value, but different kinds of pitch values.\",\n  'Yeah, I mean, for something like that, I would use pfile. Or any frame level stuff, I would use pfile.',\n  'Meaning?',\n  \"That's a, well, or something like it. ICSI has a format for frame level representation of features.\",\n  'Okay, that you could call, that you would tie into this representation with like an ID.',\n  \"Right, or there's a particular way in XML to refer to external resources.\",\n  'Okay.',\n  \"So you would say, refer to this external file. So that external file wouldn't be in.\",\n  \"But what's the advantage of doing that versus just putting it into this format? More compact.\",\n  \"Which I think is better. I mean, if you did it at this... You don't want to do it with that. Anything at frame level, you had better in code binary or it's going to be really painful.\",\n  'I mean, I like text formats. You can always gzip them and decompress them on the fly if space is really...',\n  'I was thinking the advantage is that we can share this with other people.',\n  \"You're talking about gigabyte-sized files. You're going to actually run out of space in your file system for one file. Right, because you have a two gigabyte limit on most OSs.\",\n  \"I would say, okay, so frame level is probably not a good idea. But for phone level stuff, it's perfectly, like phones or syllables.\",\n  'Phones are every five frames though, so something like that.',\n  \"But most of the frames are actually not speech. So, you know, people don't.\",\n  'Yeah, but we actually.',\n  \"Look at it, words times the average number of phones in an English word is, I don't know, five maybe. So look at it, number of words times five.\",\n  'So you mean pause phones take up a lot of the, long pause phones.',\n  'Yeah. Exactly.',\n  \"Okay, that's true, but you do have to keep them in there.\",\n  \"So I think it's debatable whether you want to do phone level in the same thing, but I think anything at frame level, even p-file is too verbose. I would use something tighter than p-files.\",\n  \"Are you familiar with it? I haven't seen this particular format.\",\n  \"I've used them.\",\n  \"I don't know what their structure is. P-file for each frame is storing a vector of, Kepstroll or PLP values, right?\",\n  \"It's whatever you want, actually. So what's nice about the P file, built into it is the concept of frames, utterances, sentences, that sort of thing, that structure. And then also attached to it is an arbitrary vector of values. And it can take different types, so they don't all have to be floats. You can have integers and you can have doubles and all that sort of stuff.\",\n  'That sounds about what I...',\n  \"Right, and it has a header format that... describes it to some extent. so the only problem with it is it's actually storing the utterance numbers and the frame numbers in the file even though they're always sequential and so it does waste a lot of space. but it's still a lot tighter than ASCII and we have a lot of tools already to deal with it.\",\n  'Okay is there some documentation?',\n  \"Yeah there's a ton of it man pages and source code and I mean, that sounds good.\",\n  \"I was just looking for something. I'm not a database person, but something sort of standard enough that, you know, if we start using this, we can give it out, other people can work on it.\",\n  \"Yeah, it's not standard. I mean, it's something that we developed at ICSI.\",\n  \"But it's been used here.\",\n  \"But it's been used here, and, you know, we have a well-configured system that you can distribute for free.\",\n  'I mean, it must be the equivalent of whatever you guys use to store your... computed features in, right?',\n  \"Yeah, we have... Actually, we use a generalization of the sphere format. But, yeah, so there's something like that, but it's...\",\n  'And I think, what does HTK do for features? Or does it even have a concept of features?',\n  \"I mean, Entropic has their own feature format. It's called, like, S. SD or something, SF or something like that.\",\n  \"I'm just wondering, would it be worthwhile to use that instead?\",\n  'Yeah, this is exactly the kind of decision.',\n  \"People don't typically share this kind of stuff, right?\",\n  'They generate their own.',\n  \"Actually, we've done this stuff on prosodics, and three or four places have asked for those prosodic files, and we just have an ASCII output of frame by frame, which is fine. But it gets unwieldy to go in and query these files with really huge files.\",\n  'Right.',\n  \"And we could do it. I was just thinking if there's something that...\",\n  \"And again, if you have a... Where all the frame values are. If you have a two-hour long meeting, that's going to...\",\n  \"They're quite large.\",\n  \"Yeah, I mean, they'd be enormous.\",\n  \"These are for 10-minute switchboard conversations. Right. So it's doable. It's just that you can only store a feature vector frame by frame, and it doesn't have any kind of...\",\n  'Is the sharing part of this? a pretty important consideration, or is that just sort of a nice thing to have?',\n  \"I don't know enough about what we're going to do with the data, but I thought it would be good to get something that other people can use or adopt for their own kinds of encoding. We have to make some decision about what to do, and especially for the prosody work, what it ends up being is you get... features from the signal and of course those change every time your alignments change. so you rerun a recognizer you want to recompute your features and then keep the database up to date. or you change a word or you change a utterance boundary segment which is going to happen a lot. and so I wanted something where all of this can be done in an elegant way and that if somebody wants to try something or compute something else that it can be done flexibly. it doesn't have to be pretty it just has to be easy to use.\",\n  \"And yeah the other thing we should look at atlas the nist thing and see if they have anything at that level. i mean i'm not sure what to do about this with atlas because they chose a different route. i chose something that there are sort of two choices your file format can know about know that you're talking about language and speech, which is what I chose, and time, or your file format can just be a graph representation. And then the application has to impose the structure on top. So what it looked like Atlas chose is they chose the other way, which was their file format is just nodes and links, and you have to interpret what they mean yourself.\",\n  'And why did you not choose that?',\n  \"Because I knew that we were doing speech, and I thought it was better, if you're looking at a raw file, for the tags to say it's an utterance, as opposed to the tag to say, It's a link.\",\n  'Okay. But other than that, are they compatible? I mean, you could sort of...',\n  \"Yeah, they're reasonably compatible.\",\n  'You could probably translate between them.',\n  \"Yeah, that's... So...\",\n  'So, well, the other thing is if we choose to use Atlas, which maybe we should just do, we should just throw this out before we invest a lot of time in it.',\n  \"So this is what the meeting's about, just sort of how to... Because we need to come up with a database like this just to do our work. And I actually don't care as long as it's something useful to other people, what we choose. So maybe it's... If you have any idea of how to choose, because I don't.\",\n  \"Yeah. I chose this for a couple reasons. One of them is that it's easy to parse. You don't need a full XML parser. It's very easy to just write a Perl script to parse it.\",\n  'As long as each tag is on one line.',\n  'Exactly, exactly, which I always do.',\n  'And you can have as much information in the tag as you want, right?',\n  'Well, I have it structured, right? So each tag has only particular items that it can take.',\n  'But you can add to those structures if you have more information.',\n  \"So what NIST would say is that instead of doing this, you would say something like link start equals some node ID and equals some other node ID. And then type would be utterance. So it's very similar.\",\n  \"So why would it be a waste to do it this way if it's similar enough that we can always?\",\n  \"Probably wouldn't be a waste. it would mean that at some point if we wanted to switch We just have to write a translator.\",\n  \"Fancy since they're developing a big.\",\n  \"I don't think that's a big deal.\",\n  \"They're developing a big infrastructure And so it seems to me that if we want to use that we might as well go directly to what they're doing rather than Yes, see that's the question.\",\n  'I mean how stable is there? are they ready to go?',\n  \"I looked at it. The last time I looked at it was a while ago, probably a year ago, when we first started talking about this. And at that time, at least, it was still not very complete. And so, specifically, they didn't have any external format representation at that time. They just had sort of conceptual node annotated transcription graph, which I really liked. And that's exactly what this stuff is based on. Since then, they've developed their own external file format, which is... You know, this sort of thing. And apparently they've also developed a lot of tools, but I haven't looked at them. Maybe I should.\",\n  'Would the tools run on something like this, if you can translate them in any way?',\n  'What would worry me is that maybe we might miss a little detail. That would make it very difficult to translate from one to the other.',\n  'They already have or will have tools that everybody else will be using.',\n  'Yeah, we might as well.',\n  'It would be crazy to do something separate.',\n  \"So I'll take a closer look at it.\",\n  \"That would really be the question, just what you feel is, in the long run, the best thing. Because once we start doing this, we don't actually have enough time to probably have to rehash it out again.\",\n  'The other thing... The other way that I sort of established this was as easy translation to and from the transcriber format. Right, right.',\n  'I mean, I like this. This is sort of intuitively easy to actually read, as easy as it could be. But I suppose that as long as they have a type here that specifies...',\n  \"It's almost the same. The point with this, though, is that you can't really add any supplementary information. Right, so if you suddenly decide that you want...\",\n  'You have to make a different type.',\n  \"Yeah, you'd have to make a different type.\",\n  \"So, well, if you look at it, and I guess in my mind, I don't know enough, Jane would know better about the types of annotations, but I imagine that those are things that would, well, you guys mentioned this, that could span any, it could be in its own channel, it could span time boundaries of any type, it could be instantaneous, things like that. And then from the recognition side, we have backtraces at the phone level. If it can handle that, it could handle states or whatever. And then at the prosody level, we have frame sort of like actual feature files, like these p-files or anything like that. And that's sort of the world of things that I... And then we have the aligned channels, of course.\",\n  'It seems to me you want to keep the frame level stuff separate.',\n  \"Yeah, I definitely agree, and I wanted to find actually a nicer format or maybe a more compact format than what we used before, just because you've got 10 channels or whatever and two hours of a meeting.\",\n  'Now, how would you represent multiple speakers in this framework? You would just represent them as... You would have like a speaker tag or something?',\n  \"There's a speaker tag up at the top which identifies them, and then the way I had it is each churn or each utterance, I don't even remember now, had a speaker ID tag attached to it. Okay. In this format, you would have a different tag, which would be linked to the link. Yeah. So somewhere else you would have another thing that would be, let's see, would it be a node or a link? And so this one would have an ID, is link 74 or something like that. And then somewhere up here you would have a link that, you know, was referencing L74 and had speaker Adam, you know, or something like that.\",\n  'I mean, yeah, channel is what the channelized output is.',\n  \"This isn't quite right. I have to look at it again.\"]}"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"def segment_transcript(max_input_length, transcript, tokenizer):\n  def split_line(line, role, tokenizer):\n    splits = []\n\n    sentences = sent_tokenize(line)\n    split_idx = len(sentences)//2\n    line1 = \" \".join(sentences[:split_idx]) + '.\\n'\n    line2 = role + \": \" + \" \".join(sentences[split_idx:])\n\n    for line in [line1, line2]:\n      if len(tokenizer.encode(line)) >= max_input_length:\n        splits += split_line(line, role, tokenizer)\n      else:\n        splits.append(line)\n\n    return splits\n\n  roles = transcript['roles']\n  attendees = sorted(list(set(roles)))\n  utterances = transcript['utterances']\n  segmented_transcript = [\"\"]\n\n  for role, utterance in zip(roles, utterances):\n    line = role + ': ' + utterance + '\\n'\n    # TODO remove short lines?\n    tokenized_line = tokenizer.encode(line)\n\n    if len(tokenized_line)>=max_input_length:\n        line_splits = split_line(line, role, tokenizer)\n    else:\n        line_splits = [line]\n\n    for line_split in line_splits:\n        tokenized = tokenizer.encode(segmented_transcript[-1]+line_split)\n        if len(tokenized)>=max_input_length:\n            segmented_transcript.append(line_split)\n        else:\n            segmented_transcript[-1] += line_split\n\n  return segmented_transcript, attendees","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-06-26T19:49:13.858146Z","iopub.execute_input":"2025-06-26T19:49:13.858416Z","iopub.status.idle":"2025-06-26T19:49:13.873052Z","shell.execute_reply.started":"2025-06-26T19:49:13.858388Z","shell.execute_reply":"2025-06-26T19:49:13.872306Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\nsegmented_transcript_long, attendees_long = segment_transcript(512, input_preprocessed, tokenizer)\nsegmented_transcript_avg, _ = segment_transcript(768, input_preprocessed, tokenizer)\nsegmented_transcript_short, _ = segment_transcript(1024, input_preprocessed, tokenizer)\n\nprint(segmented_transcript_short[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:13.873782Z","iopub.execute_input":"2025-06-26T19:49:13.874083Z","iopub.status.idle":"2025-06-26T19:49:14.334577Z","shell.execute_reply.started":"2025-06-26T19:49:13.874056Z","shell.execute_reply":"2025-06-26T19:49:14.333083Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Speaker 0: It didn't move yesterday either when I started it. I don't know if it doesn't like both of them.\nSpeaker 1: Channel 3?\nSpeaker 0: You know, I discovered something yesterday on these wireless ones. You can tell if it's picking up breath noise and stuff.\nSpeaker 1: Yeah, it has a little indicator on it, on the AF.\nSpeaker 0: So if you breathe under, breathe, and then you see AF go off, then you know it's picking up your mouth noise.\nSpeaker 2: Oh, that's good, because we have a lot of breath tests. In fact, if you listen to just the channels of people not talking, it's like...\nSpeaker 1: What, did you see Hannibal recently or something? Exactly.\nSpeaker 2: Very disconcerting. Okay. So I was going to try to get out of here in half an hour because I really appreciate people coming. And the main thing that I was going to ask people to help with today is to... give input on what kinds of database format we should use in starting to link up things like word transcripts and annotations of word transcripts, so anything that transcribers or discourse coders would ever put in the signal with time marks for like words and phone boundaries and all the stuff we get out of the forced alignments in the recognizer. So we have this, I think a starting point is clearly the channelized output of Dave Gelbart's program, which Don brought a copy of.\nSpeaker 1: Yeah, I'm familiar with that. I mean, I sort of already have developed an XML format for this sort of stuff. And so the only question, is it the sort of thing that you want to use or not? Have you looked at that? I mean, I had a web page up. Right.\nSpeaker 2: So I actually mostly need to be able to link up or... It's a question both of what the representation is.\nSpeaker 1: I guess I am going to be standing up and drawing on the board.\nSpeaker 2: Okay, yeah, so you should definitely.\nSpeaker 1: So it definitely had that as a concept. So it has a single timeline, and then you can have lots of different sections, each of which have IDs attached to it, and then you can refer from other sections to those IDs if you want to. So that you start with a timeline tag. Timeline. And then you have a bunch of times. I don't remember exactly what my notation was, but...\nSpeaker 3: Right, right.\nSpeaker 1: Yeah, t equals 1.32. And then I also had optional things like accuracy, and then id equals t10. And then I also wanted to be able to not specify specifically what the time was and just have a stamp. Yeah, so these are arbitrary assigned by a program, not by a user. So you have a whole bunch of those. And then somewhere further down, you might have something like an utterance tag, which has start equals T17, end equals T18. So what that's saying is we know it starts at this particular time. We don't know when it ends. Okay. Right? But it ends at this T18, which may be somewhere else. We say there's another utterance. We don't know what the time actually is, but we know that it's the same time as this end time. Yeah, 38, whatever you want.\nSpeaker 3: So you're essentially defining a lattice.\nSpeaker 1: Yes, exactly. And then these also have IDs. Right? So you could have some sort of other tag later in the file. that would be something like... Oh, I don't know. Noise type equals door slam, you know. And then you could either say time equals a particular time mark, or you could do other sorts of references. Or you might have a prosody. Prosody, right? D, T?\nSpeaker 2: It's an O instead of an I, but the D is good.\nSpeaker 1: You like the D? That's a good D. So you could have some sort of type here, and then you could have the utterance that it's referring to could be U17 or something like that.\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(len(segmented_transcript_short))\nprint(len(segmented_transcript_avg))\nprint(len(segmented_transcript_long))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:14.341372Z","iopub.execute_input":"2025-06-26T19:49:14.341638Z","iopub.status.idle":"2025-06-26T19:49:14.346653Z","shell.execute_reply.started":"2025-06-26T19:49:14.341617Z","shell.execute_reply":"2025-06-26T19:49:14.345809Z"}},"outputs":[{"name":"stdout","text":"5\n7\n10\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def summarize(input_text, summarizer):\n  summarization = summarizer(input_text)[0][\"summary_text\"].strip()\n  return summarization\n\ndef generate_summary(segmented_transcript, summarizer):\n  summarized_segments = [summarize(transcript_segment, summarizer) for transcript_segment in segmented_transcript]\n  return \" \".join(summarized_segments)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:14.348243Z","iopub.execute_input":"2025-06-26T19:49:14.348507Z","iopub.status.idle":"2025-06-26T19:49:15.715397Z","shell.execute_reply.started":"2025-06-26T19:49:14.348479Z","shell.execute_reply":"2025-06-26T19:49:15.714488Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# summary_short = generate_summary(segmented_transcript_short, summarizer)\n# summary_short","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:15.716282Z","iopub.execute_input":"2025-06-26T19:49:15.716589Z","iopub.status.idle":"2025-06-26T19:49:16.773346Z","shell.execute_reply.started":"2025-06-26T19:49:15.716555Z","shell.execute_reply":"2025-06-26T19:49:16.772146Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# summary_avg = generate_summary(segmented_transcript_avg, summarizer)\n# summary_avg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:16.774486Z","iopub.execute_input":"2025-06-26T19:49:16.774838Z","iopub.status.idle":"2025-06-26T19:49:17.630410Z","shell.execute_reply.started":"2025-06-26T19:49:16.774798Z","shell.execute_reply":"2025-06-26T19:49:17.629045Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# -------------HERE-----------------\nsummary_long = generate_summary(segmented_transcript_long, summarizer)\nprint(summary_long)\n\n# ----------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:17.631242Z","iopub.execute_input":"2025-06-26T19:49:17.631486Z","iopub.status.idle":"2025-06-26T19:49:27.039648Z","shell.execute_reply.started":"2025-06-26T19:49:17.631464Z","shell.execute_reply":"2025-06-26T19:49:27.038902Z"}},"outputs":[{"name":"stdout","text":"Speaker 1 and Speaker 2 are going to help Speaker 1 create a database format to link up word transcripts and annotations of word transcripts. Speaker 1 has already developed an XML format for this sort of thing. Speaker 2 is going to be standing up and drawing on the board. Speaker 1 describes a timeline with a single timeline, lots of different sections, each of which has IDs attached to it, and then you can refer from other sections to those IDs. The IDs are arbitrary assigned by a program, not by a user. There are also optional things like accuracy, id and stamping. Speaker 1 would use an existing way of doing an alignment for phone level. Speaker 2 would use a binary representation. Speaker 3 would use some file that configures how much information he wants in his XML or something. Speaker 1 doesn't want to do it. Speaker 1 would use pfile for word level data. Speaker 2 would use ICSI format for frame level representation of features. Speaker 3 would use text format. Speaker 0 was thinking the advantage is that we can share this with other people. Speaker 1 thinks it's better to do it in code binary. Speaker 2, Speaker 3, Speaker 1 and Speaker 0 discuss the P-file format. They discuss the differences between p-file and p-files. Speaker 1 prefers p- files as they store the utterance numbers and the frame numbers in the file. Speaker 1 developed a database system at ICSI. Speaker 2 wants to share it with other people. Speaker 1 and Speaker 3 use a generalization of the sphere format for computed features. Entropic has their own feature format called S. SD or something, SF. Speaker 2 wants to share the data with other people. Speaker 1 wants to look at Atlas' file format. Atlas chose the other way and their file format is just nodes and links, and you have to interpret what they mean yourself. Speaker 2 and Speaker 1 think they're reasonably compatible. Speaker 1 chose to use Atlas because it's easy to parse. It's very easy to write a Perl script to parse it. NIST would say that instead of doing this, you would say something like link start equals some node ID and node ID equals some other node ID. Speaker 1 looked at the project last time a year ago, when it was still not very complete. Since then, they've developed their own external file format and developed a lot of tools. Speaker 1 is worried that they might miss a little detail and it would be difficult to translate from one format to the other. Speaker 3 thinks it's crazy to do something separate. Speaker 2 explains to Speaker 3 how he would like to represent speakers in a new format. Speaker 1 explains how he used to represent them as a speaker tag and each utterance had a speaker ID tag attached to it. In the new format each speaker would have a different tag, which would be linked to the link.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('punkt_tab')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:27.040404Z","iopub.execute_input":"2025-06-26T19:49:27.040678Z","iopub.status.idle":"2025-06-26T19:49:27.227615Z","shell.execute_reply.started":"2025-06-26T19:49:27.040646Z","shell.execute_reply":"2025-06-26T19:49:27.226733Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nimport datetime\n\ndef fix_entities(text):\n    entity_map = {\n        \"Speaker\": \"SPEAKER\",\n        \"Organization\": \"ORGANIZATION\",\n        \"Project\": \"PROJECT\",\n        \"Location\": \"LOCATION\",\n        \"Annotator\": \"ANNOTATOR\",\n        \"Url\": \"URL\",\n        \"Number\": \"NUMBER\",\n        \"Password\": \"PASSWORD\",\n        \"Phone\": \"PHONE\",\n        \"Path\": \"PATH\",\n        \"Email\": \"EMAIL\",\n        \"Other\": \"OTHER\"\n    }\n\n    for key, val in entity_map.items():\n        pattern = re.compile(rf\"{key}\\s*(\\d+)\", re.IGNORECASE)\n        text = pattern.sub(lambda m: f\"{val}{m.group(1)}\", text)\n\n    return text\n\n\ndef create_minutes(summary):\n    summary = fix_entities(summary.strip())\n    if not summary:\n        return \"- No summary available.\"\n    sentences = sent_tokenize(summary)\n    minutes = \"\\n\".join([f\"- {sent}\" for sent in sentences])\n    return minutes\n\n\ndef format_minutes(attendees, minutes):\n    tday = datetime.date.today().strftime(\"%B %d, %Y\")  # e.g. March 28, 2025\n    att = \", \".join(attendees) if attendees else \"N/A\"\n    return f\"\"\"DATE: {tday}\nATTENDEES: {att}\n\n\nSUMMARY\n{minutes}\n\n\nMinuted by: Team Synapse\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:27.228571Z","iopub.execute_input":"2025-06-26T19:49:27.228834Z","iopub.status.idle":"2025-06-26T19:49:27.235007Z","shell.execute_reply.started":"2025-06-26T19:49:27.228813Z","shell.execute_reply":"2025-06-26T19:49:27.233993Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ----------------------HERE---------------------\nminutes_text = format_minutes(attendees_long, create_minutes(summary_long))\n\nprint(minutes_text)\n\n# -----------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:27.235988Z","iopub.execute_input":"2025-06-26T19:49:27.236283Z","iopub.status.idle":"2025-06-26T19:49:27.261744Z","shell.execute_reply.started":"2025-06-26T19:49:27.236255Z","shell.execute_reply":"2025-06-26T19:49:27.260901Z"}},"outputs":[{"name":"stdout","text":"DATE: June 26, 2025\nATTENDEES: Speaker 0, Speaker 1, Speaker 2, Speaker 3\n\n\nSUMMARY\n- SPEAKER1 and SPEAKER2 are going to help SPEAKER1 create a database format to link up word transcripts and annotations of word transcripts.\n- SPEAKER1 has already developed an XML format for this sort of thing.\n- SPEAKER2 is going to be standing up and drawing on the board.\n- SPEAKER1 describes a timeline with a single timeline, lots of different sections, each of which has IDs attached to it, and then you can refer from other sections to those IDs.\n- The IDs are arbitrary assigned by a program, not by a user.\n- There are also optional things like accuracy, id and stamping.\n- SPEAKER1 would use an existing way of doing an alignment for phone level.\n- SPEAKER2 would use a binary representation.\n- SPEAKER3 would use some file that configures how much information he wants in his XML or something.\n- SPEAKER1 doesn't want to do it.\n- SPEAKER1 would use pfile for word level data.\n- SPEAKER2 would use ICSI format for frame level representation of features.\n- SPEAKER3 would use text format.\n- SPEAKER0 was thinking the advantage is that we can share this with other people.\n- SPEAKER1 thinks it's better to do it in code binary.\n- SPEAKER2, SPEAKER3, SPEAKER1 and SPEAKER0 discuss the P-file format.\n- They discuss the differences between p-file and p-files.\n- SPEAKER1 prefers p- files as they store the utterance numbers and the frame numbers in the file.\n- SPEAKER1 developed a database system at ICSI.\n- SPEAKER2 wants to share it with other people.\n- SPEAKER1 and SPEAKER3 use a generalization of the sphere format for computed features.\n- Entropic has their own feature format called S. SD or something, SF.\n- SPEAKER2 wants to share the data with other people.\n- SPEAKER1 wants to look at Atlas' file format.\n- Atlas chose the other way and their file format is just nodes and links, and you have to interpret what they mean yourself.\n- SPEAKER2 and SPEAKER1 think they're reasonably compatible.\n- SPEAKER1 chose to use Atlas because it's easy to parse.\n- It's very easy to write a Perl script to parse it.\n- NIST would say that instead of doing this, you would say something like link start equals some node ID and node ID equals some other node ID.\n- SPEAKER1 looked at the project last time a year ago, when it was still not very complete.\n- Since then, they've developed their own external file format and developed a lot of tools.\n- SPEAKER1 is worried that they might miss a little detail and it would be difficult to translate from one format to the other.\n- SPEAKER3 thinks it's crazy to do something separate.\n- SPEAKER2 explains to SPEAKER3 how he would like to represent speakers in a new format.\n- SPEAKER1 explains how he used to represent them as a speaker tag and each utterance had a speaker ID tag attached to it.\n- In the new format each speaker would have a different tag, which would be linked to the link.\n\n\nMinuted by: Team Synapse\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Deploy as an API using FastAPI + ngrok","metadata":{}},{"cell_type":"code","source":"!pip install fastapi uvicorn pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:27.262548Z","iopub.execute_input":"2025-06-26T19:49:27.262822Z","iopub.status.idle":"2025-06-26T19:49:31.561464Z","shell.execute_reply.started":"2025-06-26T19:49:27.262802Z","shell.execute_reply":"2025-06-26T19:49:31.560577Z"}},"outputs":[{"name":"stdout","text":"Collecting fastapi\n  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\nCollecting uvicorn\n  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\nCollecting pyngrok\n  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.11.0a2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.29.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.2)\nDownloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\nSuccessfully installed fastapi-0.115.14 pyngrok-7.2.11 starlette-0.46.2 uvicorn-0.34.3\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from fastapi import FastAPI,File, UploadFile, Form\nimport uvicorn\nfrom pyngrok import ngrok\nimport nest_asyncio\nfrom pydantic import BaseModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:31.562403Z","iopub.execute_input":"2025-06-26T19:49:31.562657Z","iopub.status.idle":"2025-06-26T19:49:32.108299Z","shell.execute_reply.started":"2025-06-26T19:49:31.562635Z","shell.execute_reply":"2025-06-26T19:49:32.107602Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"nest_asyncio.apply()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:32.109178Z","iopub.execute_input":"2025-06-26T19:49:32.109426Z","iopub.status.idle":"2025-06-26T19:49:32.113138Z","shell.execute_reply.started":"2025-06-26T19:49:32.109405Z","shell.execute_reply":"2025-06-26T19:49:32.112370Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"!ngrok authtoken 2v6HSyefAGrtn5M23OyJHPLbwZV_3bcw9gHUCCbfA55k4ZuNT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:32.113857Z","iopub.execute_input":"2025-06-26T19:49:32.114142Z","iopub.status.idle":"2025-06-26T19:49:33.614264Z","shell.execute_reply.started":"2025-06-26T19:49:32.114120Z","shell.execute_reply":"2025-06-26T19:49:33.613236Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"app = FastAPI(title=\"Transcript Processing API\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:33.615308Z","iopub.execute_input":"2025-06-26T19:49:33.615630Z","iopub.status.idle":"2025-06-26T19:49:33.620577Z","shell.execute_reply.started":"2025-06-26T19:49:33.615600Z","shell.execute_reply":"2025-06-26T19:49:33.619639Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Define a request model to accept the transcript text\nclass TranscriptRequest(BaseModel):\n    transcript: str\n\n# Define a response model for both summary and meeting minutes\nclass SummaryMinutesResponse(BaseModel):\n    summary: str\n    minutes: str\n\n@app.post(\"/summaryMinutes\", response_model=SummaryMinutesResponse)\ndef process_transcript(request: TranscriptRequest):\n    print(f\"Transcript Content: {transcript[:10]}\")\n    \n    transcript_lines = request.transcript.splitlines()\n    \n    parsed_transcript = parse_transcript_by_speaker(transcript_lines)\n    \n    preprocessed = preprocess_transcript(parsed_transcript)\n    \n    segmented_transcript, attendees = segment_transcript(512, preprocessed, tokenizer)\n    \n    summary_text = generate_summary(segmented_transcript, summarizer)\n    \n    minutes_text = format_minutes(attendees, create_minutes(summary_text))\n\n    response = SummaryMinutesResponse(summary=summary_text, minutes=minutes_text)\n\n    print(\"=== API Output ===\")\n    print(f\"Summary:\\n{response.summary}\\n\")\n    print(f\"Minutes:\\n{response.minutes}\\n\")\n    \n    # Return the summary and minutes as a JSON response\n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:33.621351Z","iopub.execute_input":"2025-06-26T19:49:33.621649Z","iopub.status.idle":"2025-06-26T19:49:33.641138Z","shell.execute_reply.started":"2025-06-26T19:49:33.621625Z","shell.execute_reply":"2025-06-26T19:49:33.640504Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"public_url = ngrok.connect(8000).public_url\nprint(f\"Public API URL: {public_url}\")\nuvicorn.run(app, host=\"0.0.0.0\", port=8000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T19:49:33.642150Z","iopub.execute_input":"2025-06-26T19:49:33.642444Z","execution_failed":"2025-06-26T22:05:18.429Z"}},"outputs":[{"name":"stderr","text":"INFO:     Started server process [31]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"Public API URL: https://52e0-34-83-211-31.ngrok-free.app\nTranscript Content: [\"\\ufeffSpeaker 1: Yeah, we had a long discussion about how easy we want to make it for people to bleep things out.  Morgan wants to make it hard.  I didn't even check yesterday.  \", '', \"Speaker 0: It didn't move yesterday either when I started it.  I don't know if it doesn't like both of them.  \", '', 'Speaker 1: Channel 3?  ', '', \"Speaker 0: You know, I discovered something yesterday on these wireless ones.  You can tell if it's picking up breath noise and stuff.  \", '', 'Speaker 1: Yeah, it has a little indicator on it, on the AF.  ', '']\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"=== API Output ===\nSummary:\nMorgan wants to make it easy for people to bleep things out. Speaker 1 has developed an XML format for this sort of thing. Speaker 0 discovered that wireless speakers can tell if they are picking up breath noise and stuff. Speaker 2 is going to draw on the board. Speaker 1 describes a timeline with a single timeline, lots of different sections, each of which has IDs attached to it, and then you can refer from other sections to those IDs. The IDs are arbitrary assigned by a program, not by a user. There are also optional things like accuracy, id and stamping. Speaker 1 would use an existing way of doing an alignment for phone level. Speaker 2 would use a binary representation. Speaker 3 would use some file that configures how much information he wants in his XML or something. Speaker 1 doesn't want to do it. Speaker 1 would use pfile for word level data. Speaker 2 would use ICSI format for frame level representation of features. Speaker 3 would use text format. Speaker 0 was thinking the advantage is that we can share this with other people. Speaker 1 thinks it's better to do it in code binary. Speaker 2, Speaker 3, Speaker 1 and Speaker 0 discuss the P-file format. They discuss the differences between p-file and p-files. Speaker 1 prefers p- files as they store the utterance numbers and the frame numbers in the file. Speaker 1 developed a database system at ICSI. Speaker 2 wants to share it with other people. Speaker 1 and Speaker 3 use a generalization of the sphere format for computed features. Entropic has their own feature format called S. SD or something, SF. Speaker 2 wants to share the data with other people. Speaker 1 wants to look at Atlas' file format. Atlas chose the other way and their file format is just nodes and links, and you have to interpret what they mean yourself. Speaker 2 and Speaker 1 think they're reasonably compatible. Speaker 1 chose to use Atlas because it's easy to parse. It's very easy to write a Perl script to parse it. NIST would say that instead of doing this, you would say something like link start equals some node ID and node ID equals some other node ID. Speaker 1 looked at the project last time a year ago, when it was still not very complete. Since then, they've developed their own external file format and developed a lot of tools. Speaker 1 is worried that they might miss a little detail and it would be difficult to translate from one format to the other. Speaker 3 thinks it's crazy to do something separate. Speaker 2 explains to Speaker 3 how he would like to represent speakers in a new format. Speaker 1 explains how he used to represent them as a speaker tag and each utterance had a speaker ID tag attached to it. In the new format each speaker would have a different tag, which would be linked to the link.\n\nMinutes:\nDATE: June 26, 2025\nATTENDEES: Speaker 0, Speaker 1, Speaker 2, Speaker 3\n\n\nSUMMARY\n- Morgan wants to make it easy for people to bleep things out.\n- SPEAKER1 has developed an XML format for this sort of thing.\n- SPEAKER0 discovered that wireless speakers can tell if they are picking up breath noise and stuff.\n- SPEAKER2 is going to draw on the board.\n- SPEAKER1 describes a timeline with a single timeline, lots of different sections, each of which has IDs attached to it, and then you can refer from other sections to those IDs.\n- The IDs are arbitrary assigned by a program, not by a user.\n- There are also optional things like accuracy, id and stamping.\n- SPEAKER1 would use an existing way of doing an alignment for phone level.\n- SPEAKER2 would use a binary representation.\n- SPEAKER3 would use some file that configures how much information he wants in his XML or something.\n- SPEAKER1 doesn't want to do it.\n- SPEAKER1 would use pfile for word level data.\n- SPEAKER2 would use ICSI format for frame level representation of features.\n- SPEAKER3 would use text format.\n- SPEAKER0 was thinking the advantage is that we can share this with other people.\n- SPEAKER1 thinks it's better to do it in code binary.\n- SPEAKER2, SPEAKER3, SPEAKER1 and SPEAKER0 discuss the P-file format.\n- They discuss the differences between p-file and p-files.\n- SPEAKER1 prefers p- files as they store the utterance numbers and the frame numbers in the file.\n- SPEAKER1 developed a database system at ICSI.\n- SPEAKER2 wants to share it with other people.\n- SPEAKER1 and SPEAKER3 use a generalization of the sphere format for computed features.\n- Entropic has their own feature format called S. SD or something, SF.\n- SPEAKER2 wants to share the data with other people.\n- SPEAKER1 wants to look at Atlas' file format.\n- Atlas chose the other way and their file format is just nodes and links, and you have to interpret what they mean yourself.\n- SPEAKER2 and SPEAKER1 think they're reasonably compatible.\n- SPEAKER1 chose to use Atlas because it's easy to parse.\n- It's very easy to write a Perl script to parse it.\n- NIST would say that instead of doing this, you would say something like link start equals some node ID and node ID equals some other node ID.\n- SPEAKER1 looked at the project last time a year ago, when it was still not very complete.\n- Since then, they've developed their own external file format and developed a lot of tools.\n- SPEAKER1 is worried that they might miss a little detail and it would be difficult to translate from one format to the other.\n- SPEAKER3 thinks it's crazy to do something separate.\n- SPEAKER2 explains to SPEAKER3 how he would like to represent speakers in a new format.\n- SPEAKER1 explains how he used to represent them as a speaker tag and each utterance had a speaker ID tag attached to it.\n- In the new format each speaker would have a different tag, which would be linked to the link.\n\n\nMinuted by: Team Synapse\n\nINFO:     197.161.60.104:0 - \"POST /summaryMinutes HTTP/1.1\" 200 OK\n","output_type":"stream"}],"execution_count":null}]}